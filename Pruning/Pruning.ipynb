{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Pruning.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"BZHgZvx9Vl5P"},"source":["# Pruning process\n","\n","In this code, we'll apply the Pruning compression technique to a pre-trained model. We use the Google's Cloud TPUs in order to train the models and the data used in this project is stored in Google's Cloud Platform. "]},{"cell_type":"markdown","metadata":{"id":"2I4tbNkpV0XI"},"source":["## Conecting to Google Cloud Storage (**GCS**)\n","\n","We use a private bucket to store the data and models, if you want access to this bucket please email us at  cafajar@uis.edu.co."]},{"cell_type":"code","metadata":{"id":"aBCMDnxICuCe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635364917895,"user_tz":300,"elapsed":31978,"user":{"displayName":"Carlos Fajardo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03627228217968593322"}},"outputId":"43a0a5a9-059e-4378-c3c0-6aeaefad925b"},"source":["import uuid\n","from google.colab import auth\n","\n","project_id = 'fine-program-318215'\n","bucket_name = 'colab-sample-bucket-' + str(uuid.uuid1())\n","\n","auth.authenticate_user()\n","!gcloud config set project {project_id}\n","\n","!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n","!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n","!apt -qq update\n","!apt -qq install gcsfuse\n","\n","!mkdir folderOnColab\n","!gcsfuse --implicit-dirs test_cloud_andres folderOnColab\n","\n","!ls folderOnColab/"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Updated property [core/project].\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  2537  100  2537    0     0  90607      0 --:--:-- --:--:-- --:--:-- 90607\n","OK\n","38 packages can be upgraded. Run 'apt list --upgradable' to see them.\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-460\n","Use 'apt autoremove' to remove it.\n","The following NEW packages will be installed:\n","  gcsfuse\n","0 upgraded, 1 newly installed, 0 to remove and 38 not upgraded.\n","Need to get 10.8 MB of archives.\n","After this operation, 23.2 MB of additional disk space will be used.\n","Selecting previously unselected package gcsfuse.\n","(Reading database ... 155062 files and directories currently installed.)\n","Preparing to unpack .../gcsfuse_0.36.0_amd64.deb ...\n","Unpacking gcsfuse (0.36.0) ...\n","Setting up gcsfuse (0.36.0) ...\n","2021/10/27 20:01:57.237878 Using mount point: /content/folderOnColab\n","2021/10/27 20:01:57.246151 Opening GCS connection...\n","2021/10/27 20:01:57.412119 Mounting file system \"test_cloud_andres\"...\n","2021/10/27 20:01:57.450049 File system has been successfully mounted.\n","h5  models  tfrecords  zioApgo9\n"]}]},{"cell_type":"code","metadata":{"id":"CS7XhCvZC_Gx","executionInfo":{"status":"ok","timestamp":1635364924844,"user_tz":300,"elapsed":1513,"user":{"displayName":"Carlos Fajardo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03627228217968593322"}}},"source":["import os\n","import h5py\n","import sys\n","import tempfile\n","import zipfile\n","import numpy as np \n","import random as rn\n","import tensorflow as tf\n","import tensorflow.keras as keras\n","import matplotlib.pyplot as plt \n","import sklearn.metrics as sklm\n","\n","#Reproducibility\n","#seed = 0\n","#os.environ['PYTHONHASHSEED'] = '0'\n","#np.random.seed(seed)\n","#rn.seed(seed)\n","#tf.random.set_seed(seed)\n","\n","from tensorflow.keras import backend as K\n","from tensorflow.keras import optimizers \n","from tensorflow.keras import layers\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, classification_report\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qy1qSp-XWE5o"},"source":["## Enabling the TPU\n","First, check in the Notebook settings and select TPU from the Hardware Accelerator drop-down."]},{"cell_type":"code","metadata":{"id":"O0MWZE4tC8Pm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635364941452,"user_tz":300,"elapsed":11972,"user":{"displayName":"Carlos Fajardo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03627228217968593322"}},"outputId":"9da181e3-b61e-485c-d127-17116545132d"},"source":["resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')  # TPU detection\n","tf.config.experimental_connect_to_cluster(resolver)\n","tf.tpu.experimental.initialize_tpu_system(resolver)\n","print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n","\n","tpu_strategy = tf.distribute.TPUStrategy(resolver)"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Clearing out eager caches\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Clearing out eager caches\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Initializing the TPU system: grpc://10.51.26.98:8470\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Initializing the TPU system: grpc://10.51.26.98:8470\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Finished initializing TPU system.\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Finished initializing TPU system.\n"]},{"output_type":"stream","name":"stdout","text":["All devices:  [LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:0', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:1', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:2', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:3', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:4', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:5', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:6', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:7', device_type='TPU')]\n","INFO:tensorflow:Found TPU system:\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Found TPU system:\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Num TPU Cores: 8\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Num TPU Cores: 8\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Num TPU Workers: 1\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Num TPU Workers: 1\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"]}]},{"cell_type":"markdown","metadata":{"id":"B2iNe4ovWKEj"},"source":["## Input data\n","Our input data is stored on Google Cloud Storage. We've stored our input data in TFRecord files. We have five files equally divided to allow for a \n","cross-validation training, if needed."]},{"cell_type":"code","metadata":{"id":"rHO5FQ_fQLga","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635364965543,"user_tz":300,"elapsed":1489,"user":{"displayName":"Carlos Fajardo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03627228217968593322"}},"outputId":"bd0bcad5-b791-4c39-ee06-4aa96bc3a21d"},"source":["AUTO = tf.data.experimental.AUTOTUNE                    # Allows for optimizations\n","batch_size = 16 * tpu_strategy.num_replicas_in_sync\n","fold_no = 1                                             # If not doing cross-validation, \n","                                                        # the first set is for validation and the others for training\n","\n","gcs_pattern = 'gs://test_cloud_andres/tfrecords/11k/kfolds/*.tfrecords'\n","filenames = tf.io.gfile.glob(gcs_pattern)\n","validation_fns = filenames.pop(fold_no-1)\n","train_fns = filenames\n","test_fns = tf.io.gfile.glob('gs://test_cloud_andres/tfrecords/11k/test_2200_max3.tfrecords')\n","\n","print('Train TFRecords:',train_fns)\n","print('Validation TFRecord:',validation_fns)\n","print('Test TFRecord:',test_fns)\n","\n","def parse_tfrecord(example):\n","  features = {'X': tf.io.FixedLenFeature([2049,], tf.float32),  # ECG signal\n","              'Y': tf.io.FixedLenFeature([1,]   , tf.int64  ),  # class\n","             }\n","  example = tf.io.parse_single_example(example, features)\n","  return example['X'], example['Y']-1\n","\n","def load_dataset(filenames):\n","  records = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n","  return records.map(parse_tfrecord, num_parallel_calls=AUTO)\n","\n","train_dataset = load_dataset(train_fns).repeat().shuffle(2000000).batch(batch_size).prefetch(AUTO) \n","val_dataset   = load_dataset(validation_fns).batch(batch_size).prefetch(AUTO) \n","test_dataset  = load_dataset(test_fns).batch(batch_size).prefetch(AUTO) "],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Train TFRecords: ['gs://test_cloud_andres/tfrecords/11k/kfolds/train_1760_max3_f2.tfrecords', 'gs://test_cloud_andres/tfrecords/11k/kfolds/train_1760_max3_f3.tfrecords', 'gs://test_cloud_andres/tfrecords/11k/kfolds/train_1760_max3_f4.tfrecords', 'gs://test_cloud_andres/tfrecords/11k/kfolds/train_1760_max3_f5.tfrecords']\n","Validation TFRecord: gs://test_cloud_andres/tfrecords/11k/kfolds/train_1760_max3_f1.tfrecords\n","Test TFRecord: ['gs://test_cloud_andres/tfrecords/11k/test_2200_max3.tfrecords']\n"]}]},{"cell_type":"markdown","metadata":{"id":"LzOKcQC-WMYv"},"source":["Calculating steps for training"]},{"cell_type":"code","metadata":{"id":"5E02Snazdnzq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635364970084,"user_tz":300,"elapsed":213,"user":{"displayName":"Carlos Fajardo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03627228217968593322"}},"outputId":"7115eda3-794c-4cc0-e15f-c9a500c5dc69"},"source":["\"\"\"\n","The number of signals in each TFRecord file was previously calculated and is\n","hard-coded in this cell to avoid loading the data (expensive).\n","\"\"\"\n","\n","test_size = 507443\n","test_steps = int(np.ceil(test_size/batch_size))\n","\n","def get_steps(fold_no, batch_size):\n","  total_size = 2048149\n","  if fold_no == 1:\n","    val_size = 410780\n","    return  int(np.ceil((total_size - val_size)/batch_size)), int(np.ceil(val_size/batch_size))\n","  elif fold_no == 2:\n","    val_size = 410539\n","    return  int(np.ceil((total_size - val_size)/batch_size)), int(np.ceil(val_size/batch_size))\n","  elif fold_no == 3:\n","    val_size = 409318\n","    return  int(np.ceil((total_size - val_size)/batch_size)), int(np.ceil(val_size/batch_size))\n","  elif fold_no == 4:\n","    val_size = 407967\n","    return  int(np.ceil((total_size - val_size)/batch_size)), int(np.ceil(val_size/batch_size))\n","  elif fold_no == 5:\n","    val_size = 409545\n","    return  int(np.ceil((total_size - val_size)/batch_size)), int(np.ceil(val_size/batch_size))\n","\n","train_steps, val_steps = get_steps(fold_no, batch_size)\n","\n","print(train_steps)\n","print(val_steps)\n","print(test_steps)"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["12792\n","3210\n","3965\n"]}]},{"cell_type":"markdown","metadata":{"id":"XlF__ktCWOoa"},"source":["Loading ground-truth values to memory for latter evaluation\n"]},{"cell_type":"code","metadata":{"id":"G0ZOIdBkJ2tu"},"source":["y_true = []\n","for signal in test_dataset:\n","  Y = signal[1].numpy()\n","  y_true.extend(Y)\n","y_true = np.array(y_true)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2my1HARu5EFd"},"source":["## Model\n","We pruned several models, for more details see our paper. In this case we prune the model with 4,455 parameters after being distilled."]},{"cell_type":"code","metadata":{"id":"gspYtNxiuDYp","executionInfo":{"status":"ok","timestamp":1635365079428,"user_tz":300,"elapsed":4212,"user":{"displayName":"Carlos Fajardo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03627228217968593322"}}},"source":["with tpu_strategy.scope():  # Model is created in the TPUStrategy so it will train on the TPU\n","  def zeropad(x, filters):  # Pad zeros to match dimensions\n","    pad = K.zeros_like(x)\n","    assert (filters % pad.shape[2]) == 0\n","    num_repeat = filters // pad.shape[2]\n","    for i in range(num_repeat - 1):\n","        x = K.concatenate([x, pad], axis=2)\n","    return x \n","\n","  def basic_block(x_in, pool_size, strides, filters, kernel_size, DP):\n","      y = layers.MaxPooling1D(pool_size=pool_size, strides=strides, padding='same')(x_in)\n","      y = layers.Lambda(zeropad, arguments={'filters':filters})(y) \n","\n","      x = layers.BatchNormalization(axis=-1)(x_in)\n","      x = layers.ReLU()(x) \n","      x = layers.Conv1D(filters=filters, kernel_size=kernel_size, padding='same')(x)\n","      x = layers.BatchNormalization(axis=-1)(x)\n","      x = layers.ReLU()(x)\n","      x = layers.Dropout(DP)(x)\n","      x = layers.Conv1D(filters=filters, kernel_size=kernel_size, padding='same')(x)\n","      x = layers.AveragePooling1D(pool_size=pool_size, strides=strides, padding='same')(x)\n","      x = layers.Add()([y,x])\n","      return x\n","\n","  # Training parameters\n","  res_blocks  = 8\n","  initial_filters = 2\n","  s_j = 8\n","\n","  kernel_size = 16\n","  input_shape = (2049, 1)\n","  DP = 0.2\n","  pool_size = 2\n","  strides = 2\n","  k = 0\n","\n","  ##############################################################################\n","  ################################# MODEL ######################################\n","\n","  filters = initial_filters*(2**k) # Modify the outputs of the conv layers\n","  input_signal = tf.keras.Input(shape=input_shape, name='ECG_signal')\n","  x = layers.Conv1D(filters=filters, kernel_size=kernel_size, padding='same')(input_signal)\n","  x = layers.BatchNormalization(axis=-1)(x)\n","  x = layers.ReLU()(x)\n","\n","  y = layers.MaxPooling1D(pool_size=pool_size, strides=strides, padding='same')(x)\n","  y = layers.Lambda(zeropad, arguments={'filters':filters})(y) \n","\n","  x = layers.Conv1D(filters=filters, kernel_size=kernel_size, padding='same')(x)\n","  x = layers.BatchNormalization(axis=-1)(x)\n","  x = layers.ReLU()(x)\n","  x = layers.Dropout(DP)(x)\n","  x = layers.Conv1D(filters=filters, kernel_size=kernel_size, strides=strides, padding='same')(x)\n","  x = layers.Add()([y,x])\n","\n","  for i in range(res_blocks):\n","      if i%s_j == 0:\n","          filters = initial_filters*(2**k)\n","          k = k + 1 \n","          strides = 2\n","          x = basic_block(x, pool_size, strides, filters, kernel_size, DP)\n","      else:\n","          strides = 1  \n","          x = basic_block(x, pool_size, strides, filters, kernel_size, DP)\n","\n","  x = layers.BatchNormalization(axis=-1)(x)        \n","  x = layers.ReLU()(x)\n","  x = layers.Flatten()(x)\n","  outputs = layers.Dense(3)(x)\n","  model = tf.keras.Model(inputs=input_signal, outputs=outputs)\n","  model.compile(\n","      optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),\n","      loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","      metrics = [tf.keras.metrics.SparseCategoricalAccuracy()],\n","      steps_per_execution = 2400  # between 2 and steps_per_epoch\n","      )"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"3AUuJAw3x76F","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635365083461,"user_tz":300,"elapsed":215,"user":{"displayName":"Carlos Fajardo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03627228217968593322"}},"outputId":"0a7b4d1e-ac36-4d81-b974-6a9657ddbd83"},"source":["model.summary()"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","ECG_signal (InputLayer)         [(None, 2049, 1)]    0                                            \n","__________________________________________________________________________________________________\n","conv1d (Conv1D)                 (None, 2049, 2)      34          ECG_signal[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization (BatchNorma (None, 2049, 2)      8           conv1d[0][0]                     \n","__________________________________________________________________________________________________\n","re_lu (ReLU)                    (None, 2049, 2)      0           batch_normalization[0][0]        \n","__________________________________________________________________________________________________\n","conv1d_1 (Conv1D)               (None, 2049, 2)      66          re_lu[0][0]                      \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 2049, 2)      8           conv1d_1[0][0]                   \n","__________________________________________________________________________________________________\n","re_lu_1 (ReLU)                  (None, 2049, 2)      0           batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","max_pooling1d (MaxPooling1D)    (None, 1025, 2)      0           re_lu[0][0]                      \n","__________________________________________________________________________________________________\n","dropout (Dropout)               (None, 2049, 2)      0           re_lu_1[0][0]                    \n","__________________________________________________________________________________________________\n","lambda (Lambda)                 (None, 1025, 2)      0           max_pooling1d[0][0]              \n","__________________________________________________________________________________________________\n","conv1d_2 (Conv1D)               (None, 1025, 2)      66          dropout[0][0]                    \n","__________________________________________________________________________________________________\n","add (Add)                       (None, 1025, 2)      0           lambda[0][0]                     \n","                                                                 conv1d_2[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 1025, 2)      8           add[0][0]                        \n","__________________________________________________________________________________________________\n","re_lu_2 (ReLU)                  (None, 1025, 2)      0           batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","conv1d_3 (Conv1D)               (None, 1025, 2)      66          re_lu_2[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 1025, 2)      8           conv1d_3[0][0]                   \n","__________________________________________________________________________________________________\n","re_lu_3 (ReLU)                  (None, 1025, 2)      0           batch_normalization_3[0][0]      \n","__________________________________________________________________________________________________\n","dropout_1 (Dropout)             (None, 1025, 2)      0           re_lu_3[0][0]                    \n","__________________________________________________________________________________________________\n","max_pooling1d_1 (MaxPooling1D)  (None, 513, 2)       0           add[0][0]                        \n","__________________________________________________________________________________________________\n","conv1d_4 (Conv1D)               (None, 1025, 2)      66          dropout_1[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_1 (Lambda)               (None, 513, 2)       0           max_pooling1d_1[0][0]            \n","__________________________________________________________________________________________________\n","average_pooling1d (AveragePooli (None, 513, 2)       0           conv1d_4[0][0]                   \n","__________________________________________________________________________________________________\n","add_1 (Add)                     (None, 513, 2)       0           lambda_1[0][0]                   \n","                                                                 average_pooling1d[0][0]          \n","__________________________________________________________________________________________________\n","batch_normalization_4 (BatchNor (None, 513, 2)       8           add_1[0][0]                      \n","__________________________________________________________________________________________________\n","re_lu_4 (ReLU)                  (None, 513, 2)       0           batch_normalization_4[0][0]      \n","__________________________________________________________________________________________________\n","conv1d_5 (Conv1D)               (None, 513, 2)       66          re_lu_4[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_5 (BatchNor (None, 513, 2)       8           conv1d_5[0][0]                   \n","__________________________________________________________________________________________________\n","re_lu_5 (ReLU)                  (None, 513, 2)       0           batch_normalization_5[0][0]      \n","__________________________________________________________________________________________________\n","dropout_2 (Dropout)             (None, 513, 2)       0           re_lu_5[0][0]                    \n","__________________________________________________________________________________________________\n","max_pooling1d_2 (MaxPooling1D)  (None, 513, 2)       0           add_1[0][0]                      \n","__________________________________________________________________________________________________\n","conv1d_6 (Conv1D)               (None, 513, 2)       66          dropout_2[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_2 (Lambda)               (None, 513, 2)       0           max_pooling1d_2[0][0]            \n","__________________________________________________________________________________________________\n","average_pooling1d_1 (AveragePoo (None, 513, 2)       0           conv1d_6[0][0]                   \n","__________________________________________________________________________________________________\n","add_2 (Add)                     (None, 513, 2)       0           lambda_2[0][0]                   \n","                                                                 average_pooling1d_1[0][0]        \n","__________________________________________________________________________________________________\n","batch_normalization_6 (BatchNor (None, 513, 2)       8           add_2[0][0]                      \n","__________________________________________________________________________________________________\n","re_lu_6 (ReLU)                  (None, 513, 2)       0           batch_normalization_6[0][0]      \n","__________________________________________________________________________________________________\n","conv1d_7 (Conv1D)               (None, 513, 2)       66          re_lu_6[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_7 (BatchNor (None, 513, 2)       8           conv1d_7[0][0]                   \n","__________________________________________________________________________________________________\n","re_lu_7 (ReLU)                  (None, 513, 2)       0           batch_normalization_7[0][0]      \n","__________________________________________________________________________________________________\n","dropout_3 (Dropout)             (None, 513, 2)       0           re_lu_7[0][0]                    \n","__________________________________________________________________________________________________\n","max_pooling1d_3 (MaxPooling1D)  (None, 513, 2)       0           add_2[0][0]                      \n","__________________________________________________________________________________________________\n","conv1d_8 (Conv1D)               (None, 513, 2)       66          dropout_3[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_3 (Lambda)               (None, 513, 2)       0           max_pooling1d_3[0][0]            \n","__________________________________________________________________________________________________\n","average_pooling1d_2 (AveragePoo (None, 513, 2)       0           conv1d_8[0][0]                   \n","__________________________________________________________________________________________________\n","add_3 (Add)                     (None, 513, 2)       0           lambda_3[0][0]                   \n","                                                                 average_pooling1d_2[0][0]        \n","__________________________________________________________________________________________________\n","batch_normalization_8 (BatchNor (None, 513, 2)       8           add_3[0][0]                      \n","__________________________________________________________________________________________________\n","re_lu_8 (ReLU)                  (None, 513, 2)       0           batch_normalization_8[0][0]      \n","__________________________________________________________________________________________________\n","conv1d_9 (Conv1D)               (None, 513, 2)       66          re_lu_8[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_9 (BatchNor (None, 513, 2)       8           conv1d_9[0][0]                   \n","__________________________________________________________________________________________________\n","re_lu_9 (ReLU)                  (None, 513, 2)       0           batch_normalization_9[0][0]      \n","__________________________________________________________________________________________________\n","dropout_4 (Dropout)             (None, 513, 2)       0           re_lu_9[0][0]                    \n","__________________________________________________________________________________________________\n","max_pooling1d_4 (MaxPooling1D)  (None, 513, 2)       0           add_3[0][0]                      \n","__________________________________________________________________________________________________\n","conv1d_10 (Conv1D)              (None, 513, 2)       66          dropout_4[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_4 (Lambda)               (None, 513, 2)       0           max_pooling1d_4[0][0]            \n","__________________________________________________________________________________________________\n","average_pooling1d_3 (AveragePoo (None, 513, 2)       0           conv1d_10[0][0]                  \n","__________________________________________________________________________________________________\n","add_4 (Add)                     (None, 513, 2)       0           lambda_4[0][0]                   \n","                                                                 average_pooling1d_3[0][0]        \n","__________________________________________________________________________________________________\n","batch_normalization_10 (BatchNo (None, 513, 2)       8           add_4[0][0]                      \n","__________________________________________________________________________________________________\n","re_lu_10 (ReLU)                 (None, 513, 2)       0           batch_normalization_10[0][0]     \n","__________________________________________________________________________________________________\n","conv1d_11 (Conv1D)              (None, 513, 2)       66          re_lu_10[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_11 (BatchNo (None, 513, 2)       8           conv1d_11[0][0]                  \n","__________________________________________________________________________________________________\n","re_lu_11 (ReLU)                 (None, 513, 2)       0           batch_normalization_11[0][0]     \n","__________________________________________________________________________________________________\n","dropout_5 (Dropout)             (None, 513, 2)       0           re_lu_11[0][0]                   \n","__________________________________________________________________________________________________\n","max_pooling1d_5 (MaxPooling1D)  (None, 513, 2)       0           add_4[0][0]                      \n","__________________________________________________________________________________________________\n","conv1d_12 (Conv1D)              (None, 513, 2)       66          dropout_5[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_5 (Lambda)               (None, 513, 2)       0           max_pooling1d_5[0][0]            \n","__________________________________________________________________________________________________\n","average_pooling1d_4 (AveragePoo (None, 513, 2)       0           conv1d_12[0][0]                  \n","__________________________________________________________________________________________________\n","add_5 (Add)                     (None, 513, 2)       0           lambda_5[0][0]                   \n","                                                                 average_pooling1d_4[0][0]        \n","__________________________________________________________________________________________________\n","batch_normalization_12 (BatchNo (None, 513, 2)       8           add_5[0][0]                      \n","__________________________________________________________________________________________________\n","re_lu_12 (ReLU)                 (None, 513, 2)       0           batch_normalization_12[0][0]     \n","__________________________________________________________________________________________________\n","conv1d_13 (Conv1D)              (None, 513, 2)       66          re_lu_12[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_13 (BatchNo (None, 513, 2)       8           conv1d_13[0][0]                  \n","__________________________________________________________________________________________________\n","re_lu_13 (ReLU)                 (None, 513, 2)       0           batch_normalization_13[0][0]     \n","__________________________________________________________________________________________________\n","dropout_6 (Dropout)             (None, 513, 2)       0           re_lu_13[0][0]                   \n","__________________________________________________________________________________________________\n","max_pooling1d_6 (MaxPooling1D)  (None, 513, 2)       0           add_5[0][0]                      \n","__________________________________________________________________________________________________\n","conv1d_14 (Conv1D)              (None, 513, 2)       66          dropout_6[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_6 (Lambda)               (None, 513, 2)       0           max_pooling1d_6[0][0]            \n","__________________________________________________________________________________________________\n","average_pooling1d_5 (AveragePoo (None, 513, 2)       0           conv1d_14[0][0]                  \n","__________________________________________________________________________________________________\n","add_6 (Add)                     (None, 513, 2)       0           lambda_6[0][0]                   \n","                                                                 average_pooling1d_5[0][0]        \n","__________________________________________________________________________________________________\n","batch_normalization_14 (BatchNo (None, 513, 2)       8           add_6[0][0]                      \n","__________________________________________________________________________________________________\n","re_lu_14 (ReLU)                 (None, 513, 2)       0           batch_normalization_14[0][0]     \n","__________________________________________________________________________________________________\n","conv1d_15 (Conv1D)              (None, 513, 2)       66          re_lu_14[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_15 (BatchNo (None, 513, 2)       8           conv1d_15[0][0]                  \n","__________________________________________________________________________________________________\n","re_lu_15 (ReLU)                 (None, 513, 2)       0           batch_normalization_15[0][0]     \n","__________________________________________________________________________________________________\n","dropout_7 (Dropout)             (None, 513, 2)       0           re_lu_15[0][0]                   \n","__________________________________________________________________________________________________\n","max_pooling1d_7 (MaxPooling1D)  (None, 513, 2)       0           add_6[0][0]                      \n","__________________________________________________________________________________________________\n","conv1d_16 (Conv1D)              (None, 513, 2)       66          dropout_7[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_7 (Lambda)               (None, 513, 2)       0           max_pooling1d_7[0][0]            \n","__________________________________________________________________________________________________\n","average_pooling1d_6 (AveragePoo (None, 513, 2)       0           conv1d_16[0][0]                  \n","__________________________________________________________________________________________________\n","add_7 (Add)                     (None, 513, 2)       0           lambda_7[0][0]                   \n","                                                                 average_pooling1d_6[0][0]        \n","__________________________________________________________________________________________________\n","batch_normalization_16 (BatchNo (None, 513, 2)       8           add_7[0][0]                      \n","__________________________________________________________________________________________________\n","re_lu_16 (ReLU)                 (None, 513, 2)       0           batch_normalization_16[0][0]     \n","__________________________________________________________________________________________________\n","conv1d_17 (Conv1D)              (None, 513, 2)       66          re_lu_16[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_17 (BatchNo (None, 513, 2)       8           conv1d_17[0][0]                  \n","__________________________________________________________________________________________________\n","re_lu_17 (ReLU)                 (None, 513, 2)       0           batch_normalization_17[0][0]     \n","__________________________________________________________________________________________________\n","dropout_8 (Dropout)             (None, 513, 2)       0           re_lu_17[0][0]                   \n","__________________________________________________________________________________________________\n","max_pooling1d_8 (MaxPooling1D)  (None, 513, 2)       0           add_7[0][0]                      \n","__________________________________________________________________________________________________\n","conv1d_18 (Conv1D)              (None, 513, 2)       66          dropout_8[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_8 (Lambda)               (None, 513, 2)       0           max_pooling1d_8[0][0]            \n","__________________________________________________________________________________________________\n","average_pooling1d_7 (AveragePoo (None, 513, 2)       0           conv1d_18[0][0]                  \n","__________________________________________________________________________________________________\n","add_8 (Add)                     (None, 513, 2)       0           lambda_8[0][0]                   \n","                                                                 average_pooling1d_7[0][0]        \n","__________________________________________________________________________________________________\n","batch_normalization_18 (BatchNo (None, 513, 2)       8           add_8[0][0]                      \n","__________________________________________________________________________________________________\n","re_lu_18 (ReLU)                 (None, 513, 2)       0           batch_normalization_18[0][0]     \n","__________________________________________________________________________________________________\n","flatten (Flatten)               (None, 1026)         0           re_lu_18[0][0]                   \n","__________________________________________________________________________________________________\n","dense (Dense)                   (None, 3)            3081        flatten[0][0]                    \n","==================================================================================================\n","Total params: 4,455\n","Trainable params: 4,379\n","Non-trainable params: 76\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","metadata":{"id":"Oi4B_nhMx8Dl","executionInfo":{"status":"ok","timestamp":1635365093531,"user_tz":300,"elapsed":1748,"user":{"displayName":"Carlos Fajardo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03627228217968593322"}}},"source":["model.load_weights('folderOnColab/models/deep_models/KD/KD_4k_distilled.h5')"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YZQJKkIrRRVZ"},"source":["## Pruning technique\n","First, we clone the model to make comparisons."]},{"cell_type":"code","metadata":{"id":"e7fIYD-XRWMO"},"source":["with tpu_strategy.scope():\n","  model_to_prune = keras.models.clone_model(model4k)\n","  model_to_prune.set_weights(model4k.get_weights())\n","  model_to_prune.compile(\n","      optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n","      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n","      steps_per_execution=2400,  # between 2 and steps_per_epoch\n","      )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uC_MVg1GQrIY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629237156538,"user_tz":300,"elapsed":3629,"user":{"displayName":"Andrés Sebastián Parra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjvu0xICzWL3lErv1doacyTGdnvDd5zEw8DoPS7HQ=s64","userId":"04099405292842376467"}},"outputId":"534dac14-6f87-4ac0-ad03-07209271318b"},"source":["!pip install tensorflow_model_optimization "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting tensorflow_model_optimization\n","  Downloading tensorflow_model_optimization-0.6.0-py2.py3-none-any.whl (211 kB)\n","\u001b[?25l\r\u001b[K     |█▌                              | 10 kB 24.7 MB/s eta 0:00:01\r\u001b[K     |███                             | 20 kB 30.0 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 30 kB 12.1 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 40 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 51 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 61 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 71 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 81 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 92 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 102 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 112 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 122 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 133 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 143 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 153 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 163 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 174 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 184 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 194 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 204 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 211 kB 5.4 MB/s \n","\u001b[?25hRequirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow_model_optimization) (0.1.6)\n","Requirement already satisfied: numpy~=1.14 in /usr/local/lib/python3.7/dist-packages (from tensorflow_model_optimization) (1.19.5)\n","Requirement already satisfied: six~=1.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow_model_optimization) (1.15.0)\n","Installing collected packages: tensorflow-model-optimization\n","Successfully installed tensorflow-model-optimization-0.6.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FAOUyVoEXCPx"},"source":["We used several *pruning_schedule* and *sparsity* values, for more details see our paper. In this case we used a *ConstantSparsity* of 50% and a low *learning_rate* for fine-tuning."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CPiZfgdm3F-A","executionInfo":{"status":"ok","timestamp":1629237160465,"user_tz":300,"elapsed":3931,"user":{"displayName":"Andrés Sebastián Parra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjvu0xICzWL3lErv1doacyTGdnvDd5zEw8DoPS7HQ=s64","userId":"04099405292842376467"}},"outputId":"5e8da9a6-9861-46a5-e697-9dc8403a860d"},"source":["import tensorflow_model_optimization as tfmot\n","\n","with tpu_strategy.scope():\n","  prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n","  pruning_params = {\n","        'pruning_schedule': tfmot.sparsity.keras.ConstantSparsity(0.5, begin_step=0, frequency=100)\n","  }\n","  model_for_pruning = prune_low_magnitude(model_to_prune, **pruning_params)             \n","  model_for_pruning.compile(\n","      optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n","      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n","      steps_per_execution=2400,  # between 2 and steps_per_epoch\n","      )\n","\n","#model_for_pruning.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py:2223: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n","  warnings.warn('`layer.add_variable` is deprecated and '\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"Ezryf9z4Xod2"},"source":["## Training pruned model"]},{"cell_type":"code","metadata":{"id":"dnVALVPY4CXv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627337529720,"user_tz":300,"elapsed":734948,"user":{"displayName":"Andrés Parra","photoUrl":"","userId":"17237001474833976907"}},"outputId":"8168d54e-72ce-41ba-d152-1147cda7da90"},"source":["callbacks_list_pruning = [\n","tfmot.sparsity.keras.UpdatePruningStep()\n","]\n","\n","history_pruning = model_for_pruning.fit(\n","    train_dataset,\n","    validation_data=val_dataset,\n","    epochs=5, \n","    steps_per_epoch=train_steps,\n","    validation_steps=val_steps,\n","    callbacks=callbacks_list_pruning, \n","    )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/5\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n","Instructions for updating:\n","The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n","Instructions for updating:\n","The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"],"name":"stderr"},{"output_type":"stream","text":["12000/12792 [===========================>..] - ETA: 17s - loss: 0.3478 - sparse_categorical_accuracy: 0.8670 WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 9.2444s vs `on_train_batch_end` time: 45.0557s). Check your callbacks.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 9.2444s vs `on_train_batch_end` time: 45.0557s). Check your callbacks.\n"],"name":"stderr"},{"output_type":"stream","text":["12792/12792 [==============================] - 321s 25ms/step - loss: 0.3463 - sparse_categorical_accuracy: 0.8675 - val_loss: 0.3133 - val_sparse_categorical_accuracy: 0.8749\n","Epoch 2/5\n","12792/12792 [==============================] - 103s 8ms/step - loss: 0.3170 - sparse_categorical_accuracy: 0.8771 - val_loss: 0.3078 - val_sparse_categorical_accuracy: 0.8778\n","Epoch 3/5\n","12792/12792 [==============================] - 103s 8ms/step - loss: 0.3103 - sparse_categorical_accuracy: 0.8795 - val_loss: 0.3049 - val_sparse_categorical_accuracy: 0.8786\n","Epoch 4/5\n","12792/12792 [==============================] - 103s 8ms/step - loss: 0.3075 - sparse_categorical_accuracy: 0.8810 - val_loss: 0.2998 - val_sparse_categorical_accuracy: 0.8806\n","Epoch 5/5\n","12792/12792 [==============================] - 103s 8ms/step - loss: 0.3053 - sparse_categorical_accuracy: 0.8819 - val_loss: 0.2975 - val_sparse_categorical_accuracy: 0.8815\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xm5I_kXWGhxY"},"source":["model_for_pruning.load_weights('folderOnColab/models/deep_models/pruning/4k_distilled.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7bL-_eBWXmlj"},"source":["## Evaluating pruned model"]},{"cell_type":"code","metadata":{"id":"wkiBz3-o4mvj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629237285495,"user_tz":300,"elapsed":116798,"user":{"displayName":"Andrés Sebastián Parra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjvu0xICzWL3lErv1doacyTGdnvDd5zEw8DoPS7HQ=s64","userId":"04099405292842376467"}},"outputId":"0f8ad65a-802d-43fa-bac6-f3f466f748b9"},"source":["(loss, acc_full) = model_for_pruning.evaluate(val_dataset, steps=val_steps, verbose=1)\n","(loss, acc_full) = model_for_pruning.evaluate(test_dataset, steps=test_steps,  verbose=1)\n","\n","y_pred = model_for_pruning.predict(test_dataset, steps=test_steps, verbose=1)\n","y_pred_bool = np.argmax(y_pred, axis=1)\n","y_true_for_f1 = y_true\n","\n","print(classification_report(y_true_for_f1, y_pred_bool))\n","print('acc',accuracy_score(y_true_for_f1, y_pred_bool))\n","print('precision',precision_score(y_true_for_f1, y_pred_bool , average=\"macro\"))\n","print('recall',recall_score(y_true_for_f1, y_pred_bool , average=\"macro\"))\n","print('f1',f1_score(y_true_for_f1, y_pred_bool , average=\"macro\"))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["3210/3210 [==============================] - 52s 16ms/step - loss: 0.2974 - sparse_categorical_accuracy: 0.8815\n","3965/3965 [==============================] - 21s 5ms/step - loss: 0.2981 - sparse_categorical_accuracy: 0.8813\n","3965/3965 [==============================] - 41s 10ms/step\n","              precision    recall  f1-score   support\n","\n","           0       0.87      0.94      0.91    287147\n","           1       0.56      0.71      0.63     17369\n","           2       0.94      0.81      0.87    202927\n","\n","    accuracy                           0.88    507443\n","   macro avg       0.79      0.82      0.80    507443\n","weighted avg       0.89      0.88      0.88    507443\n","\n","acc 0.8812891300106613\n","precision 0.7902316829757495\n","recall 0.820426543264691\n","f1 0.8004750871048488\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EB1yynuoX6l3"},"source":["We strip the pruned model to remove the pruning wrappers."]},{"cell_type":"code","metadata":{"id":"1jk2MbyptusW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627339211903,"user_tz":300,"elapsed":2221,"user":{"displayName":"Andrés Parra","photoUrl":"","userId":"17237001474833976907"}},"outputId":"d3a054e9-8a46-4d77-b6ac-36156d3268ff"},"source":["model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n","model_for_export.save('folderOnColab/models/deep_models/pruning/4k_distilled_stripped.h5', include_optimizer=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n","  category=CustomMaskWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"m-cUCPWKLkfN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627338884637,"user_tz":300,"elapsed":3775,"user":{"displayName":"Andrés Parra","photoUrl":"","userId":"17237001474833976907"}},"outputId":"f585b875-5f8e-4688-b9f3-957ccb7a9c36"},"source":["def get_gzipped_model_size(model):\n","  # Returns size of gzipped model, in bytes.\n","  import os\n","  import zipfile\n","\n","  _, keras_file = tempfile.mkstemp('.h5')\n","  model.save(keras_file, include_optimizer=False)\n","\n","  _, zipped_file = tempfile.mkstemp('.zip')\n","  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n","    f.write(keras_file)\n","\n","  return os.path.getsize(zipped_file)\n","\n","def get_params_nonzero(model):\n","    params = 0\n","    for layer in model.layers:\n","        for weight in layer.get_weights():\n","            params += np.count_nonzero(weight.flatten())\n","    return params\n","\n","print(get_params_nonzero(model4k))\n","print(get_params_nonzero(model_for_pruning))\n","print(get_params_nonzero(model_for_export))\n","print()\n","print(get_gzipped_model_size(model4k))\n","print(get_gzipped_model_size(model_for_pruning))\n","print(get_gzipped_model_size(model_for_export))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["4455\n","2324\n","2324\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n","  category=CustomMaskWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["31554\n","36337\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"],"name":"stderr"},{"output_type":"stream","text":["24834\n"],"name":"stdout"}]}]}